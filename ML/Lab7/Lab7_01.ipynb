{"nbformat":4,"nbformat_minor":0,"metadata":{"coursera":{"schema_names":["NLPC1-1"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Lab7_01.ipynb","provenance":[{"file_id":"1Y0eTOMC8_tTKwJhParXlsr7k_JNJM_An","timestamp":1630562930302},{"file_id":"https://github.com/pandyahariom/ML_Lab/blob/master/L6-LogisticRegression/tweeterAnalysis.ipynb","timestamp":1596958929064}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"ltnC-DbVFNrl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630563020207,"user_tz":-330,"elapsed":2661,"user":{"displayName":"CE045_Priyanka_Gondaliya","photoUrl":"","userId":"04462938926200923683"}},"outputId":"7351a427-269d-4a02-aed7-6016c40d5bb5"},"source":["import nltk\n","from nltk.corpus import twitter_samples \n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]}]},{"cell_type":"code","metadata":{"id":"nEw35lgbIKmJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630563029696,"user_tz":-330,"elapsed":1215,"user":{"displayName":"CE045_Priyanka_Gondaliya","photoUrl":"","userId":"04462938926200923683"}},"outputId":"3d2fc657-4977-4389-d061-4d3da2061ebc"},"source":["\n","nltk.download('twitter_samples')\n","nltk.download('stopwords')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/twitter_samples.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"K8zAwNf3IcwC","executionInfo":{"status":"ok","timestamp":1630563033873,"user_tz":-330,"elapsed":502,"user":{"displayName":"CE045_Priyanka_Gondaliya","photoUrl":"","userId":"04462938926200923683"}}},"source":["import re\n","import string\n","import numpy as np\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import TweetTokenizer"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"VOkRSbEw1Shr","executionInfo":{"status":"ok","timestamp":1630563102284,"user_tz":-330,"elapsed":480,"user":{"displayName":"CE045_Priyanka_Gondaliya","photoUrl":"","userId":"04462938926200923683"}}},"source":["#process_tweet(): cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n","def process_tweet(tweet):\n","    \"\"\"Process tweet function.\n","    Input:\n","        tweet: a string containing a tweet\n","    Output:\n","        tweets_clean: a list of words containing the processed tweet\n","\n","    \"\"\"\n","    stemmer = PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","\n","    # remove stock market tickers like $GE\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\n","    # remove old style retweet text \"RT\"\n","    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","    # remove hyperlinks\n","    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n","    # remove hashtags\n","    # only removing the hash # sign from the word\n","    tweet = re.sub(r'#', '', tweet)\n","    # tokenize tweets\n","\n","\n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n","                               reduce_len=True)\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:\n","        if(word not in stopwords_english and word not in string.punctuation):\n","          stem_word = stemmer.stem(word)\n","          tweets_clean.append(stem_word)\n","            #############################################################\n","            # 1 remove stopwords\n","            # 2 remove punctuation\n","            # 3 stemming word\n","            # 4 Add it to tweets_clean\n","\n","    return tweets_clean"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"f6v_JLxe1ZXh","executionInfo":{"status":"ok","timestamp":1630563187534,"user_tz":-330,"elapsed":490,"user":{"displayName":"CE045_Priyanka_Gondaliya","photoUrl":"","userId":"04462938926200923683"}}},"source":["#build_freqs counts how often a word in the 'corpus' (the entire set of tweets) was associated with\n","  # a positive label '1'         or \n","  # a negative label '0', \n","\n","#then builds the freqs dictionary, where each key is a (word,label) tuple, \n","\n","#and the value is the count of its frequency within the corpus of tweets.\n","\n","def build_freqs(tweets, ys):\n","    \"\"\"Build frequencies.\n","    Input:\n","        tweets: a list of tweets\n","        ys: an m x 1 array with the sentiment label of each tweet\n","            (either 0 or 1)\n","    Output:\n","        freqs: a dictionary mapping each (word, sentiment) pair to its\n","        frequency\n","    \"\"\"\n","    # Convert np array to list since zip needs an iterable.\n","    # The squeeze is necessary or the list ends up with one element.\n","    # Also note that this is just a NOP if ys is already a list.\n","    yslist = np.squeeze(ys).tolist()\n","\n","    # Start with an empty dictionary and populate it by looping over all tweets\n","    # and over all processed words in each tweet.\n","    freqs = {}\n","\n","    for y, tweet in zip(yslist, tweets):\n","        for word in process_tweet(tweet):\n","            pair = (word, y)\n","            if pair in freqs:\n","              freqs[pair] += 1\n","            else:\n","              freqs[pair] = 1\n","            \n","            #############################################################\n","            #Update the count of pair if present, set it to 1 otherwise\n","            \n","\n","    return freqs"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mt_btshaFNtJ","executionInfo":{"status":"ok","timestamp":1630563195146,"user_tz":-330,"elapsed":758,"user":{"displayName":"CE045_Priyanka_Gondaliya","photoUrl":"","userId":"04462938926200923683"}}},"source":["# select the set of positive and negative tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bFUFtkzCFNt7"},"source":["* Train test split: 20% will be in the test set, and 80% in the training set.\n"]},{"cell_type":"code","metadata":{"id":"PNp1zFFmFNuA","executionInfo":{"status":"ok","timestamp":1630563218599,"user_tz":-330,"elapsed":446,"user":{"displayName":"CE045_Priyanka_Gondaliya","photoUrl":"","userId":"04462938926200923683"}}},"source":["# split the data into two pieces, one for training and one for testing\n","#############################################################\n","test_pos = all_positive_tweets[4000:]\n","train_pos = all_positive_tweets[:4000]\n","test_neg = all_negative_tweets[4000:]\n","train_neg = all_negative_tweets[:4000]\n","train_x = train_pos + train_neg\n","test_x = test_pos + test_neg"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JyC4UAjZFNuT"},"source":["* Create the numpy array of positive labels and negative labels."]},{"cell_type":"code","metadata":{"id":"xS1wvGq7FNuX","executionInfo":{"status":"ok","timestamp":1630563272057,"user_tz":-330,"elapsed":462,"user":{"displayName":"CE045_Priyanka_Gondaliya","photoUrl":"","userId":"04462938926200923683"}}},"source":["# combine positive and negative labels\n","train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n","test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n","Final_data = all_positive_tweets+all_negative_tweets\n","data =np.append(np.ones((len(all_positive_tweets), 1)), np.zeros((len(all_negative_tweets), 1)), axis=0)\n","train_x,test_x,train_y,test_y = train_test_split(Final_data,data,test_size=0.25,random_state= 45)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RvppFUyLFNu2"},"source":["* Create the frequency dictionary using the  `build_freqs()` function.  \n","    \n"]},{"cell_type":"code","metadata":{"id":"ggrsgUh4FNu5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630563479028,"user_tz":-330,"elapsed":3577,"user":{"displayName":"CE045_Priyanka_Gondaliya","photoUrl":"","userId":"04462938926200923683"}},"outputId":"80bf5341-78c8-47d7-ce76-793cdd31312a"},"source":["# create frequency dictionary\n","#############################################################\n","freqs = build_freqs(train_x , train_y)\n","\n","# check the output\n","print(\"type(freqs) = \" + str(type(freqs)))\n","print(\"len(freqs) = \" + str(len(freqs.keys())))\n","def extract_features(tweet, freqs): \n","    word_l = process_tweet(tweet)\n","    x = np.zeros((1, 2)) \n","    for word in word_l:\n","        if((word,1) in freqs):\n","          x[0,0]+=freqs[word,1]\n","        if((word,0) in freqs):\n","          x[0,1]+=freqs[word,0]\n","    \n","    assert(x.shape == (1, 2))\n","    return x[0]\n","\n","#pred function\n","def predict_tweet(tweet):\n","    with tf.Session() as sess:\n","      saver.restore(sess,save_path='TSession')\n","      data_i=[]\n","      for t in tweet:\n","        data_i.append(extract_features(t,freqs))\n","      data_i=np.asarray(data_i)\n","      return sess.run(tf.nn.sigmoid(tf.add(tf.matmul(a=data_i,b=W,transpose_b=True),bias)))\n","    print(\"--Fail--\")\n","    return"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["type(freqs) = <class 'dict'>\n","len(freqs) = 10885\n"]}]},{"cell_type":"code","metadata":{"id":"bixjcwIOFNvP","executionInfo":{"status":"ok","timestamp":1630563505929,"user_tz":-330,"elapsed":562,"user":{"displayName":"CE045_Priyanka_Gondaliya","photoUrl":"","userId":"04462938926200923683"}}},"source":["bias=tf.Variable(np.random.randn(1),name=\"Bias\")\n","W=tf.Variable(np.random.randn(1,2),name=\"Weight\")\n"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlkgwDxIFNvq","executionInfo":{"status":"ok","timestamp":1630563528427,"user_tz":-330,"elapsed":3473,"user":{"displayName":"CE045_Priyanka_Gondaliya","photoUrl":"","userId":"04462938926200923683"}}},"source":["data=[]\n","for t in train_x:\n","  data.append(extract_features(t,freqs))\n","data=np.asarray(data)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"qrFMw4hkFNxA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630563556457,"user_tz":-330,"elapsed":496,"user":{"displayName":"CE045_Priyanka_Gondaliya","photoUrl":"","userId":"04462938926200923683"}},"outputId":"b7ea9367-f9e9-4048-dbca-90d1fa7fb3bf"},"source":["Y_hat = tf.nn.sigmoid(tf.add(tf.matmul(np.asarray(data), W,transpose_b=True), bias)) \n","ta=np.asarray(train_y)\n","Total_cost = tf.nn.sigmoid_cross_entropy_with_logits(logits = Y_hat, labels = ta) \n","print(Total_cost)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"logistic_loss:0\", shape=(7500, 1), dtype=float64)\n"]}]},{"cell_type":"code","metadata":{"id":"tBqwBhBoFNx0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630563592836,"user_tz":-330,"elapsed":9953,"user":{"displayName":"CE045_Priyanka_Gondaliya","photoUrl":"","userId":"04462938926200923683"}},"outputId":"0ce344a0-4586-4f2b-92d8-7b9e94004680"},"source":["# Gradient Descent Optimizer \n","optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.00001 ,name=\"GradientDescent\").minimize(Total_cost) \n","# Global Variables Initializer \n","init = tf.global_variables_initializer()\n","\n","saver = tf.train.Saver()\n","with tf.Session() as sess:\n","  \n","  sess.run(init)\n","  print(\"Bias\",sess.run(bias))\n","  print(\"Weight\",sess.run(W))\n","  for epoch in range(1000):\n","    sess.run(optimizer)\n","    preds=sess.run(Y_hat)\n","    acc=((preds==ta).sum())/len(train_y)\n","    Accuracy=[]\n","    repoch=False\n","    if repoch:\n","      Accuracy.append(acc)\n","    if epoch % 1000 == 0:\n","      print(\"Accuracy\",acc)\n","    saved_path = saver.save(sess, 'TSession')\n","\n","preds=predict_tweet(test_x)\n","print(preds,len(test_y))\n"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Bias [0.12732492]\n","Weight [[ 0.21032706 -0.35418891]]\n","Accuracy 0.8973333333333333\n","INFO:tensorflow:Restoring parameters from TSession\n","[[1.        ]\n"," [0.        ]\n"," [0.        ]\n"," ...\n"," [1.        ]\n"," [0.        ]\n"," [0.99999998]] 2500\n"]}]},{"cell_type":"code","metadata":{"id":"cElgAfPDFNyC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630563611181,"user_tz":-330,"elapsed":1667,"user":{"displayName":"CE045_Priyanka_Gondaliya","photoUrl":"","userId":"04462938926200923683"}},"outputId":"3a6a1b44-8973-4fbc-abd6-eb97d8444447"},"source":["def accuracy(x,y):\n","  return ((x==y).sum())/len(y)\n","\n","print(accuracy(preds,test_y))"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9168\n"]}]}]}